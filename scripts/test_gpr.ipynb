{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50599b14",
   "metadata": {},
   "source": [
    "# Test Script - Gaussian Processes Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c95807",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d8c75-048b-465c-809d-b0343b8e37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Importing Scikit-Learn Models\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, DotProduct, ExpSineSquared, Matern, RBF\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Importing Bayesian Optimizer and Acquisition Functions\n",
    "from modAL.models import BayesianOptimizer\n",
    "from modAL.acquisition import max_EI, max_PI, max_UCB\n",
    "\n",
    "# Importing Custom Acquisiton Functions and Decay Functions\n",
    "import utils\n",
    "from utils import StepWiseDecay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206e7a6",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e1ce1",
   "metadata": {},
   "source": [
    "Parameterised Cell Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb81b0-f36c-44af-9915-5e399d1d4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "fingerprint = 'morgan'\n",
    "model = 'GPR'\n",
    "file_name = \"complete_file_morgan.feather\"\n",
    "config_file_name = 'GPR.json'\n",
    "use_unified_file = True\n",
    "use_decomposition = False\n",
    "assay_limit = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddef2ef",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f063f-1870-42b3-9386-4a9306d0d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "file_path = '/data_temp/default_{}/'.format(fingerprint)\n",
    "\n",
    "# try:\n",
    "if use_unified_file == True:\n",
    "    try:\n",
    "        df = pd.read_feather(\"../data\" + file_path + file_name)\n",
    "    except:\n",
    "        df = pd.read_parquet(\"../data\" + file_path + file_name)\n",
    "    df_nan = pd.read_parquet(\"../data\" + file_path + \"assay_id/assay_id_null_file.parquet\")\n",
    "    df_assays = pd.read_parquet(\"../data\" + file_path + \"assay_id/assay_id_file.parquet\")\n",
    "\n",
    "elif use_unified_file == False:\n",
    "    df_fingerprint = pd.read_parquet(\"../\" + file_path + \"/fingerprint/{}_fingerprint_file.parquet\".format(fingerprint))\n",
    "    df = pd.read_parquet(\"../\" + file_path + \"/preprocessed/preprocessed_file.parquet\")\n",
    "    df_nan = pd.read_parquet(\"../\" + file_path + \"/assay_id/assay_id_null_file.parquet\")\n",
    "    df_assays = pd.read_parquet(\"../\" + file_path + \"/assay_id/assay_id_file.parquet\")\n",
    "else:\n",
    "    print(\"Incorrect value for 'use_unified_file' parameter passed. Please recheck.\")\n",
    "    pass\n",
    "# except:\n",
    "print(\"Data File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6193ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e8f9b",
   "metadata": {},
   "source": [
    "Identifying noisy assays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef88d7f-4cce-492c-aa89-d8e39aa92473",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan.loc[df_nan['squared_pearson_trn'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f4018",
   "metadata": {},
   "source": [
    "Removing Noisy Assays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e698e-a306-468c-bdf2-49906f197c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(nan_assays)\n",
    "for i in df_nan.loc[df_nan['squared_pearson_trn'].isnull()]['assay_id']:\n",
    "    df = df.drop(labels = df.loc[df['assay_id']==i].index)\n",
    "df.loc[df['assay_id']==303216].head()\n",
    "print('x-----x-----x-----x')\n",
    "df.loc[df['assay_id']==303260].head()\n",
    "print('x-----x-----x-----x')\n",
    "df.loc[df['assay_id']==737235].head()\n",
    "print('x-----x-----x-----x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7017f",
   "metadata": {},
   "source": [
    "Loading Config Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading JSON config file\n",
    "try:\n",
    "    with open('../config/' + config_file_name) as f:\n",
    "        params_config = json.load(f)\n",
    "        print('JSON config file for {} successfully loaded'.format(model))\n",
    "except FileNotFoundError:\n",
    "    print('Config file for model {} is missing.Resorting to default params'.format(model))\n",
    "    with open('../config/{}_default.json'.format(model)) as f:\n",
    "        params_config = json.load(f)\n",
    "\n",
    "try:\n",
    "    with open('../config/decay_values.json') as f:\n",
    "        decay_list = json.load(f)\n",
    "        decay_list = decay_list[\"decay values\"]\n",
    "except:\n",
    "    decay_list = [1.0,0.75,0.5,0.25,0.125,0.1,0.05]\n",
    "    print('Config file for decay values is missing. Resorting to default values: {}'.format(decay_list))\n",
    "\n",
    "if os.path.isdir('../models/{0}_{1}/'.format(model,fingerprint)) == False:\n",
    "    os.mkdir('../models/{0}_{1}/'.format(model,fingerprint))\n",
    "\n",
    "if os.path.isdir('../data/data_results/{0}_{1}/'.format(model,fingerprint)) == False:\n",
    "    os.mkdir('../data/data_results/{0}_{1}/'.format(model,fingerprint))\n",
    "\n",
    "if os.path.isdir('../data/data_results/{0}_{1}/plots'.format(model,fingerprint)) == False:\n",
    "    os.mkdir('../data/data_results/{0}_{1}/plots'.format(model,fingerprint))\n",
    "\n",
    "# Creating List of Classifiers\n",
    "clfs = []\n",
    "acquisition_list = []\n",
    "clf_list = []\n",
    "count = 0\n",
    "clf_list_names = []\n",
    "\n",
    "for kernel in params_config[\"kernel\"]:\n",
    "    if kernel == \"Matern()\":\n",
    "        clf_type = GPR(kernel=Matern(length_scale=params_config[\"matern_params\"][\"length_scale\"],\n",
    "                                length_scale_bounds=(params_config[\"matern_params\"][\"length_scale_bounds\"][\"low\"],params_config[\"matern_params\"][\"length_scale_bounds\"][\"high\"]),\n",
    "                                nu=params_config[\"matern_params\"][\"nu\"]),\n",
    "                    alpha=(params_config[\"alpha\"]))\n",
    "    elif kernel == \"RBF()\":\n",
    "        clf_type = GPR(kernel=RBF(length_scale=params_config[\"rbf_params\"][\"length_scale\"],\n",
    "                                length_scale_bounds=(params_config[\"rbf_params\"][\"length_scale_bounds\"][\"low\"],params_config[\"rbf_params\"][\"length_scale_bounds\"][\"high\"])),\n",
    "                    alpha=(params_config[\"alpha\"]))\n",
    "    else:\n",
    "        clf_type = GPR(alpha=(params_config[\"alpha\"]))\n",
    "    clf = {\n",
    "        \"type\" : clf_type,\n",
    "        \"name\" : kernel\n",
    "    }\n",
    "    clfs.append(clf)\n",
    "\n",
    "for function in params_config[\"acquisition\"]:\n",
    "    if function==\"max_EI\":\n",
    "        acquisition_type = max_EI\n",
    "    elif function==\"max_PI\":\n",
    "        acquisition_type = max_PI\n",
    "    elif function==\"max_UCB\":\n",
    "        acquisition_type = max_UCB\n",
    "    elif function==\"random\":\n",
    "        acquisition_type = utils.random_sampling\n",
    "    elif function==\"equivalent\":\n",
    "        acquisition_type = utils.equivalent_sampling\n",
    "    elif function==\"margin_entropy\":\n",
    "        acquisition_type = utils.margin_entropy_sampling\n",
    "    elif function==\"uncertainty_margin\":\n",
    "        acquisition_type = utils.uncertainty_margin_sampling\n",
    "    elif function==\"product_sampling\":\n",
    "        acquisition_type = utils.product_sampling\n",
    "    else:\n",
    "        acquisition_type = max_EI\n",
    "    acquisition = {\n",
    "        \"type\" : acquisition_type,\n",
    "        \"name\": function\n",
    "    }\n",
    "    acquisition_list.append(acquisition)\n",
    "\n",
    "for model_selected in clfs:\n",
    "    for ac_func in acquisition_list:\n",
    "        clf = {\n",
    "            \"model\" :model_selected[\"type\"],\n",
    "            \"model_name\" : model_selected[\"name\"],\n",
    "            \"acquisition_function\" :ac_func[\"type\"],\n",
    "            \"acquisition_function_name\" : ac_func[\"name\"]\n",
    "        }\n",
    "        clf_list.append(clf)\n",
    "        clf_list_names.append(\"model_{}_{}\".format(clf[\"model_name\"], clf[\"acquisition_function_name\"]))\n",
    "        count+=1\n",
    "\n",
    "clf_list_names = ['assay_id','subset_size_trn','total_length'] + clf_list_names\n",
    "\n",
    "num_iterations=params_config[\"iterations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760587e",
   "metadata": {},
   "source": [
    "Creating the dynamic Data Matrix to store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_trn = [[0 for i in range(len(clf_list)+3)] for j in range(len(df[\"assay_id\"].unique()+1))]\n",
    "pearson_tst = [[0 for i in range(len(clf_list)+3)] for j in range(len(df[\"assay_id\"].unique()+1))]\n",
    "\n",
    "pearson_values_graph = [[0 for i in range(len(acquisition_list))] for j in range(num_iterations)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084fddb3",
   "metadata": {},
   "source": [
    "Suppresing warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db7feb",
   "metadata": {},
   "source": [
    "## Training Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subset_sizes = []\n",
    "list_total_sizes = []\n",
    "row = 0\n",
    "column = 0\n",
    "pvg_column = 0\n",
    "pvg_row = 0\n",
    "\n",
    "# Assay for which graphs have to be generated\n",
    "# flag_first_assay = 688239\n",
    "# flag_first_assay = 517\n",
    "flag_first_assay = 70695\n",
    "\n",
    "first_start = time.time()\n",
    "\n",
    "decay_tracker = utils.StepWiseDecay(decay_list)\n",
    "count = 0\n",
    "\n",
    "for assay_id in df['assay_id'].unique():\n",
    "    if assay_id == np.int64(flag_first_assay):\n",
    "        print('Reached selected assay at time = {}'.format(time.time() - first_start))\n",
    "    df_current = df.loc[df['assay_id']==assay_id]\n",
    "    df_train = df_current.loc[df['Clustering']=='TRN']\n",
    "    df_tst = df_current.loc[df['Clustering']=='TST']\n",
    "    column=0\n",
    "    pvg_column = 0\n",
    "    # if df_train.shape[0] < 100:\n",
    "    #     continue\n",
    "    if assay_id != np.int64(flag_first_assay):\n",
    "        continue\n",
    "    start = time.time()\n",
    "\n",
    "    subset_size = int(decay_tracker.calculate(df_train.shape[0])*df_train.shape[0])\n",
    "    list_subset_sizes.append(subset_size)\n",
    "    list_total_sizes.append(df_train.shape[0])\n",
    "    if assay_id == np.int64(flag_first_assay):\n",
    "        print(\"\\nThe current assay id is {} and initialisation size is {}\\n\".format(assay_id, subset_size))\n",
    "    pearson_trn[row][column] = assay_id\n",
    "    pearson_tst[row][column] = assay_id\n",
    "    column+=1\n",
    "\n",
    "    # X = np.array(df_train.iloc[:,10:])[:subset_size]\n",
    "    # y = np.array(df_train.iloc[:,3])[:subset_size]\n",
    "\n",
    "    start_2 = time.time()\n",
    "\n",
    "    X = np.array(df_train.iloc[:,10:])\n",
    "    y = np.array(df_train.iloc[:,3])\n",
    "    if use_decomposition == True:\n",
    "        transformer = TruncatedSVD(n_components=128, random_state=42).fit(X)\n",
    "        X = transformer.transform(X)\n",
    "        with open('../models/{0}_{1}/transformer_{2}.pickle'.format(model,fingerprint,assay_id),'wb') as f:\n",
    "                pickle.dump(transformer,f)\n",
    "\n",
    "    # print(\"Subset selected in {}\".format(time.time() - start_2))\n",
    "    train_idx = np.random.choice(range(X.shape[0]), size=int(X.shape[0]*0.1), replace=False)\n",
    "    # print(X.shape[0]*0.1, df_train.shape[0])\n",
    "    \n",
    "    x_initial = X[train_idx]\n",
    "    y_initial = y[train_idx]\n",
    "    # print(\"Selecte initial training subset {}\".format(time.time() - start_2))\n",
    "    # x_initial = X[-(int(df_train.shape[0]*0.1)):-1]\n",
    "    # y_initial = y[-(int(df_train.shape[0]*0.1)):-1]\n",
    "\n",
    "    X = np.delete(X,train_idx, axis=0)\n",
    "    y = np.delete(y,train_idx)\n",
    "\n",
    "    # X = np.array(pd.DataFrame(X).reset_index(drop=True))\n",
    "    # y = np.array(pd.DataFrame(y).reset_index(drop=True))\n",
    "\n",
    "    pearson_trn[row][column] = subset_size\n",
    "    pearson_tst[row][column] = subset_size\n",
    "    column+=1\n",
    "\n",
    "    pearson_trn[row][column] = df_train.shape[0]\n",
    "    pearson_tst[row][column] = df_tst.shape[0]\n",
    "    column+=1\n",
    "\n",
    "    # if flag_first_assay==True:\n",
    "    #     [[0 for i in range(len(acquisition_list))] for j in range(num_iterations)]\n",
    "    \n",
    "    if use_unified_file==True:\n",
    "        for gpr_model in clf_list:\n",
    "            X_train = X\n",
    "            y_train = y\n",
    "            if assay_id==np.int64(flag_first_assay):\n",
    "                print(\"Data is for model {} with acc_func {}\".format(gpr_model[\"model_name\"], gpr_model[\"acquisition_function_name\"]))\n",
    "            clf = gpr_model[\"model\"]\n",
    "            # print(\"Before model creation : {}\".format(time.time() - start_2))\n",
    "            learner = BayesianOptimizer(\n",
    "                estimator=clf,\n",
    "                query_strategy=gpr_model[\"acquisition_function\"],\n",
    "                X_training=x_initial, y_training=y_initial\n",
    "            )\n",
    "            # print(\"After model creation and training {}\".format(time.time() - start_2))\n",
    "            pvg_row=0\n",
    "            for n_query in range(num_iterations):\n",
    "                if assay_id==np.int64(flag_first_assay):\n",
    "                    # first_assay = assay_id\n",
    "                    if use_decomposition == True:\n",
    "                        with open('../models/{0}_{1}/transformer_{2}.pickle'.format(model,fingerprint,assay_id),'rb') as f:\n",
    "                            transformer = pickle.load(f)\n",
    "                        predictions_first_assay = learner.predict(transformer.transform(np.array(df_train.iloc[:,10:])))\n",
    "                    else:\n",
    "                        predictions_first_assay = learner.predict(np.array(df_train.iloc[:,10:]))\n",
    "                    pearson_values_graph[pvg_row][pvg_column] = round(np.corrcoef(np.array(df_train.iloc[:,3]), predictions_first_assay)[0,1]**2,5)\n",
    "                    # print(pearson_values_graph[pvg_row][pvg_column])\n",
    "                    pvg_row+=1\n",
    "\n",
    "                    try:\n",
    "                        query_idx,query_inst = learner.query(X_train, n_instances=20)\n",
    "                    except:\n",
    "                        print(\"Encountered a case where the number of instances is lower than utility\")\n",
    "                    learner.teach(X=query_inst,y=y[query_idx])\n",
    "\n",
    "                    # X_train = np.delete(X_train,query_idx, axis=0)\n",
    "                    # y_train = np.delete(y_train,query_idx.astype(int))\n",
    "\n",
    "                # print(\"Iteration num = {} || Query index = {} || Y val = {}\".format(n_query,query_idx, y[query_idx]))\n",
    "                # print(\"Time taken to query {}\".format(time.time() - start_2))\n",
    "            # print('Training Process for acc func {} has been completed in '.format(gpr_model[\"acquisition_function_name\"],time.time()-start))    \n",
    "            # specific_model_count = 1\n",
    "            \n",
    "            with open('../models/{0}_{1}/{2}_{3}_{4}.pickle'.format(model,fingerprint,gpr_model[\"model_name\"],gpr_model[\"acquisition_function_name\"],assay_id),'wb') as f:\n",
    "                pickle.dump(learner,f)\n",
    "\n",
    "            with open('../models/{0}_{1}/{2}_{3}_{4}.pickle'.format(model,fingerprint,gpr_model[\"model_name\"],gpr_model[\"acquisition_function_name\"],assay_id),'rb') as f:\n",
    "                 learner = pickle.load(f)\n",
    "\n",
    "            if use_decomposition == True:\n",
    "                with open('../models/{0}_{1}/transformer_{2}.pickle'.format(model,fingerprint,assay_id),'rb') as f:\n",
    "                    transformer = pickle.load(f)    \n",
    "                predictions = learner.predict(transformer.transform(np.array(df_train.iloc[:,10:])))\n",
    "            else:\n",
    "                predictions = learner.predict(np.array(df_train.iloc[:,10:]))\n",
    "            result_trn = round(np.corrcoef(np.array(df_train.iloc[:,3]), predictions)[0,1]**2,5)\n",
    "\n",
    "            if use_decomposition == True:\n",
    "                with open('../models/{0}_{1}/transformer_{2}.pickle'.format(model,fingerprint,assay_id),'rb') as f:\n",
    "                    transformer = pickle.load(f)     \n",
    "                predictions = learner.predict(transformer.transform(np.array(df_tst.iloc[:,10:])))\n",
    "            else:\n",
    "                predictions = learner.predict(np.array(df_tst.iloc[:,10:]))\n",
    "            result_tst = round(np.corrcoef(np.array(df_tst.iloc[:,3]), predictions)[0,1]**2,5)\n",
    "\n",
    "            pearson_trn[row][column] = result_trn\n",
    "            pearson_tst[row][column] = result_tst\n",
    "            \n",
    "            column+=1\n",
    "            pvg_column+=1\n",
    "    else:\n",
    "        print(\"Feature is in the works.\")   \n",
    "    row+=1\n",
    "    count+=1\n",
    "    print(\"Parsed {} assays\".format(count))\n",
    "    if count >= assay_limit:\n",
    "            break\n",
    "print('Reached selected assay at time = {}'.format(time.time() - start))\n",
    "# except NameError:\n",
    "#     print('Key Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922305f",
   "metadata": {},
   "source": [
    "Displaying the time taken for model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4698892",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reached selected assay at time = {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f1486",
   "metadata": {},
   "source": [
    "Results of the selected assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_values_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pearson_trn, columns=clf_list_names).to_csv('../data/data_results/{0}_{1}/pearsons_training_set_{2}.csv'.format(model,fingerprint,num_iterations),index=False)\n",
    "pd.DataFrame(pearson_tst, columns=clf_list_names).to_csv('../data/data_results/{0}_{1}/pearsons_test_set_{2}.csv'.format(model,fingerprint,num_iterations),index=False)\n",
    "\n",
    "pearson_values_graph = pd.DataFrame(pearson_values_graph, columns=params_config[\"acquisition\"])\n",
    "pearson_values_graph.to_csv('../data/data_results/{0}_{1}/squared_pearson_{2}.csv'.format(model,fingerprint,flag_first_assay),index=False)\n",
    "\n",
    "colours = ['-r','--b',':g','^y','-.k','.c','om','xk']\n",
    "colour_count = 0\n",
    "font_custom = {\n",
    "    \"family\" : \"sans-serif\",\n",
    "    \"color\" : \"darkblue\",\n",
    "    \"size\" : \"10\"\n",
    "    }\n",
    "\n",
    "plt.title(\"Training results for assay {}\".format(flag_first_assay), fontdict=font_custom, loc='center')\n",
    "plt.xlabel(\"Iteration number\", fontdict=font_custom)\n",
    "plt.ylabel(\"Pearson's coefficient values\", fontdict=font_custom)\n",
    "\n",
    "\n",
    "for (column_name,column_contents) in pearson_values_graph.iteritems():\n",
    "    plt.plot([x for x in range(len(column_contents))],column_contents, colours[colour_count], label='{}'.format(column_name))\n",
    "    colour_count +=1\n",
    "plt.grid(color = 'lightgreen', linestyle = '--', linewidth =0.25)\n",
    "plt.legend()\n",
    "plt.savefig('../data/data_results/{0}_{1}/plots/training_cycles_{2}.jpg'.format(model,fingerprint,flag_first_assay),\n",
    "                format='jpg',\n",
    "               )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603531da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
